{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b535840a",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Part 1: Setup & Dependencies\n",
    "\n",
    "First, we import the core components.\n",
    "*   **ChatOllama**: The interface to our local Llama 3 model.\n",
    "*   **Prompts**: Templates to structure how we talk to the AI.\n",
    "*   **History**: Tools to save conversation data into a SQL database so the bot remembers us even if we restart the script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc1b5266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "# For persistent memory\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "print(\"‚úÖ Imports loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dbbd65",
   "metadata": {},
   "source": [
    "# ü§ñ Part 2: Initialize the LLM\n",
    "\n",
    "We configure the connection to Ollama.\n",
    "*   **temperature=0.8**: Set to be slightly creative.\n",
    "*   **num_predict=256**: Limits the response length to keep it concise.\n",
    "\n",
    "*(Note: 'temerature' was a typo in the original code, corrected to 'temperature' here).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d692ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model llama3.2:latest connected.\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:latest'\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    temperature=0.8,\n",
    "    num_predict=256,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model {model} connected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e0f0f",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è Part 3: The Problem (Stateless Chains)\n",
    "\n",
    "First, let's look at what happens if we **don't** use memory components.\n",
    "Standard LLM chains are \"Stateless\". They treat every input as a brand new conversation.\n",
    "\n",
    "Below, we tell the AI our name, but in the very next line, it will likely forget because we didn't pass the history back to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e07f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ User: My name is Arash...\n",
      "ü§ñ AI: Hello Arash! It's nice to meet you. As an AI Consultant, I'm sure you have a fascinating career helping organizations leverage the power of artificial intelligence.\n",
      "\n",
      "What aspects of AI are you most interested in or passionate about? Are you working with natural language processing, computer vision, machine learning, or perhaps something else?\n",
      "\n",
      "Feel free to share more about your work and interests. I'm here to listen and help if I can!\n",
      "\n",
      "üë§ User: What is my name?\n",
      "ü§ñ AI: I don't have any information about your name. I'm a large language model, I don't retain personal data or have the ability to know individual users' names unless you tell me. If you'd like to share your name with me, I'd be happy to chat with you!\n",
      "\n",
      "‚ùå Notice: The AI likely forgot the name because this chain has no memory.\n"
     ]
    }
   ],
   "source": [
    "# 1. Simple Template\n",
    "simple_template = ChatPromptTemplate.from_template(\"{prompt}\")\n",
    "simple_chain = simple_template | llm | StrOutputParser()\n",
    "\n",
    "# 2. We introduce ourselves\n",
    "print(\"üë§ User: My name is Arash...\")\n",
    "print(f\"ü§ñ AI: {simple_chain.invoke({'prompt': 'My name is Arash, I work as AI Consultant.'})}\")\n",
    "\n",
    "# 3. We ask for recall\n",
    "print(\"\\nüë§ User: What is my name?\")\n",
    "response = simple_chain.invoke({'prompt': 'what is my name?'})\n",
    "print(f\"ü§ñ AI: {response}\")\n",
    "\n",
    "print(\"\\n‚ùå Notice: The AI likely forgot the name because this chain has no memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df236c",
   "metadata": {},
   "source": [
    "# üß† Part 4: The Architecture for Memory\n",
    "\n",
    "To fix the problem above, we need a **Prompt Structure** that reserves a specific \"slot\" for history.\n",
    "\n",
    "1.  **System Message**: Defines the AI's personality.\n",
    "2.  **MessagesPlaceholder(\"history\")**: This is the magic part. It tells LangChain: *\"Insert all past messages right here before the new user input.\"*\n",
    "3.  **Human Message**: The current question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1bdd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffe6b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory-aware chain created.\n"
     ]
    }
   ],
   "source": [
    "# Define the components\n",
    "system_msg = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\")\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "\n",
    "# Create the structure with a placeholder for 'history'\n",
    "messages = [\n",
    "    system_msg,\n",
    "    MessagesPlaceholder(variable_name=\"history\"), # <--- The Memory Slot\n",
    "    human_msg\n",
    "]\n",
    "\n",
    "# Build the template and the chain\n",
    "memory_prompt = ChatPromptTemplate(messages=messages)\n",
    "memory_chain = memory_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"‚úÖ Memory-aware chain created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d94584",
   "metadata": {},
   "source": [
    "# üíæ Part 5: Wiring the Memory (SQL Database)\n",
    "\n",
    "Now we connect our chain to a SQLite database.\n",
    "*   **SQLChatMessageHistory**: Automatically saves inputs and outputs to a local file (`chat_history.db`).\n",
    "*   **RunnableWithMessageHistory**: This is the manager. It takes our `memory_chain`, looks at the `history` slot we defined, and automatically fills it with data from the database.\n",
    "\n",
    "**Key Parameters:**\n",
    "*   `input_messages_key='input'`: Matches our prompt's `{input}`.\n",
    "*   `history_messages_key='history'`: Matches our `MessagesPlaceholder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf91555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Runnable wired with SQL Database.\n"
     ]
    }
   ],
   "source": [
    "def get_session_history(session_id):\n",
    "    \"\"\"\n",
    "    Creates a connection to a SQLite DB for a specific session ID.\n",
    "    If the file doesn't exist, it creates it.\n",
    "    \"\"\"\n",
    "    connection = \"sqlite:///chat_history.db\"\n",
    "    return SQLChatMessageHistory(session_id, connection)\n",
    "\n",
    "# Wrap the chain with the history manager\n",
    "runnable_with_history = RunnableWithMessageHistory(\n",
    "    memory_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='input',\n",
    "    history_messages_key='history'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Runnable wired with SQL Database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfada139",
   "metadata": {},
   "source": [
    "# üöÄ Part 6: Testing the Persistent Chatbot\n",
    "\n",
    "Now we define a clean function `chat_with_llm` that takes a `session_id` (like a User ID).\n",
    "We will test it by:\n",
    "1.  Teaching it a fact.\n",
    "2.  Asking it to recall the fact.\n",
    "\n",
    "*Note: Since this saves to a database, if you restart this notebook and use the same 'arash' ID, it will still remember you!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2749223c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ User (arash_v1): My name is Arash, I work as AI Consultant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arash\\AppData\\Local\\ooba\\ooba\\text-generation-ui\\installer_files\\conda\\lib\\site-packages\\langchain_core\\runnables\\history.py:600: LangChainDeprecationWarning: `connection_string` was deprecated in LangChain 0.2.2 and will be removed in 1.0. Use connection instead.\n",
      "  message_history = self.get_session_history(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AI: Hello Arash! It's nice to meet you. As an AI Consultant, I'm sure you have a deep understanding of the latest advancements and applications in artificial intelligence.\n",
      "\n",
      "What can I help you with today? Do you have any questions or topics you'd like to discuss regarding AI? Or perhaps you need assistance with a project or problem you're working on?\n",
      "\n",
      "üë§ User (arash_v1): What is my job?\n",
      "ü§ñ AI: Your job, Arash, is as an AI Consultant. This means you help organizations and businesses implement and integrate artificial intelligence solutions into their operations. Your role involves assessing client needs, recommending suitable AI technologies and strategies, and guiding clients through the process of implementing these solutions.\n",
      "\n",
      "As an AI Consultant, your expertise likely spans a range of areas, including natural language processing, machine learning, computer vision, and more. You help clients to identify opportunities for automation, improve efficiency, enhance customer experiences, and gain a competitive edge in their respective industries.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your job, Arash, is as an AI Consultant. This means you help organizations and businesses implement and integrate artificial intelligence solutions into their operations. Your role involves assessing client needs, recommending suitable AI technologies and strategies, and guiding clients through the process of implementing these solutions.\\n\\nAs an AI Consultant, your expertise likely spans a range of areas, including natural language processing, machine learning, computer vision, and more. You help clients to identify opportunities for automation, improve efficiency, enhance customer experiences, and gain a competitive edge in their respective industries.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_with_llm(session_id, user_input):\n",
    "    print(f\"üë§ User ({session_id}): {user_input}\")\n",
    "    \n",
    "    output = runnable_with_history.invoke(\n",
    "        {'input': user_input},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    \n",
    "    print(f\"ü§ñ AI: {output}\\n\")\n",
    "    return output\n",
    "\n",
    "# --- Test Sequence ---\n",
    "\n",
    "user_id = 'arash_v1'\n",
    "\n",
    "# 1. Provide Context\n",
    "chat_with_llm(user_id, \"My name is Arash, I work as AI Consultant.\")\n",
    "\n",
    "# 2. Ask for Recall (This should work now!)\n",
    "chat_with_llm(user_id, \"What is my job?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
